ENVCONF=env.conf
REPDEL='|'

# =========================
# logging
# =========================

logfile() {
    local -r MESS="$1"
}

log() {
    local -r MESS="$1"
    logfile "$MESS"
    echo $MESS
}

logfail() {
    log "$1"
    exit 4
}

execute_withlog() {
    local -r CMD="$@"
    log "$CMD"
    eval $CMD
    if [ $? -ne 0 ]; then
        # log CMD again, it can preceded by bunch of logs
        log "$CMD"
        logfail "Job failed"
    fi
}

# =============================
# temporary files
# =============================

crtemp() {
  local -r TMP=`mktemp`
  echo $TMP >>$TMPSTORE
  echo $TMP
}

# =============================
# different report functions
# =============================

dir_size() {
    local -r DIR=$1
    local -r SIZE=`hdfs dfs -du -h -s $DIR | cut -d' ' -f1-2`
    echo $SIZE
}

dir_isize() {
    echo `dir_size $TMPINPUTDIR`
}

getsec() {
  echo `date  +"%s"`
}


calculatesec() {
  local -r before=$1
  local -r after=`getsec`
  echo $(expr $after - $before)
}

getdate() {
    echo `date +"%Y-%m-%d %H-%M-%S"`
}

tsp() {
    local -r MESS="$1"
    local -r LEN=$2
    local -r OUT=`printf "%-${LEN}s $REPDEL" "$MESS"`
    echo "$OUT"
}

NAMELEN=20
DATELEN=20
SECLEN=10
SIZELEN=10


printline() {
    echo -n $REPDEL >>$REPORTFILE
    while true; do
        [ -z "$1" ] && break
        O=`tsp "$1" $2`
        echo -n "$O" >>$REPORTFILE
        shift 2
    done
    echo >>$REPORTFILE
}

testbeg() {
    local -r subtest=$1
    if [ ! -f $REPORTFILE ]; then
        printline TEST $NAMELEN SUBTEST $NAMELEN SIZE $SIZELEN START $DATELEN END $DATELEN TIME/SEC $SECLEN
    fi 
    local -r BEG="$subtest,`getdate`,`getsec`"
    echo $BEG
}

testend() {
    IFS=',' read -r subtest begdate begsec <<<$@
    printline $TESTNAME $NAMELEN $subtest $NAMELEN "`dir_isize`" $SIZELEN "$begdate" $DATELEN "`getdate`" $DATELEN "`calculatesec $begsec`" $SECLEN
}

# ========================
# misc script utilties
# ========================

remove_tmpoutput() {
  rmr_hdfs $TMPOUTPUTDIR
}

remove_tmpoutput1() {
  rmr_hdfs $TMPOUTPUT1DIR
}


remove_tmp() {
  rmr_hdfs $TMPBASEDIR
}

required_var() {
    local -r VARIABLE=$1
    [ -z "${!VARIABLE}" ] && logfail "Need to set environment variable $VARIABLE"    
}

required_listofvars() {
    while true; do
        var=$1
        [ -z "$var" ] && break
        required_var $var
        shift
    done
}

required_par() {
    local -r PAR=$1
    [ -z "${!PAR}" ] && logfail "$PAR not set in $ENVCONF"
    [ "${!PAR}" == "-" ] && logfail "$PAR not set in $ENVCONF"
}

required_listofpars() {
    while true; do
        par=$1
        [ -z "$par" ] && break
        required_par $par
        shift
    done
}

log_listofpars() {
    while true; do
        par=$1
        [ -z "$par" ] && break
        val=${!par}
        log "$par=$val"
        shift
    done
}

verify_pars() {
  required_listofpars $@
  log_listofpars $@
}  


existfile() {
    local -r FILENAME=$1
    [ -f $FILENAME ] || logfail "$FILENAME does not exist"
}

existexefile() {
    local -r FILENAME=$1
    [ -x $FILENAME ] || logfail "$FILENAME is not executable"
}

existdir() {
    local -r DIRNAME=$1
    [ -d $DIRNAME ] || logfail "$DIRNAME does not exist"
}


setenv() {
    source $ENVRC
    source $CUSTOMRC
}

onthelist() {
    local -r word=$1
    local -r list=$2
    for w in ${list//,/ }; do
        [ $w == $word ] &&  return
    done
    logfail "$word is not on the list $list"
}

getconfvar() {
    declare -A arr

    # read file line by line. Field separator is "="
    while IFS='=' read -r k v; do
        [ -z "$k" ] && continue
        [ -z "$v" ] && continue
        arr[$k]=$v
    done <$ENVCONF
    res=""
    while true; do
        key=$1
        [ -z "$key" ] && break
        val=${arr[$BENCHSIZE.$key]}
        if [ -z "$val" ]; then val=${arr[$key]}; fi

        # check global variable
        if [ -z "$val" ]; then 
          # replace . with _
          read EVAR <<< `echo $key | tr . _`
          val="${!EVAR}"        
        fi
        if [ -z "$val" ]; then logfile "$1 variable not found in $ENVCONF or $EVAR does not exist"; val="-";  fi
        res="$res $val"
        shift
    done
    echo $res
}

# ===========================
# repeating spark
# ===========================

spark_prepare() {
    local -r CLASS=$1
    local -r BEGTEST=`testbeg prepare`
    read -r NUM_EXAMPLES_LINEAR NUM_FEATURES_LINEAR <<< `getconfvar examples features`
    verify_pars NUM_EXAMPLES_LINEAR NUM_FEATURES_LINEAR

    OPTIONS="--numExamples $NUM_EXAMPLES_LINEAR \
             --numFeatures $NUM_FEATURES_LINEAR \
             --dataPath $TMPINPUTDIR
            "
    sparkbenchjar $1 $OPTIONS
    testend $BEGTEST
}

spark_run() {
    local -r NAME=$1
    local -r CLASS=$2
    local -r BEGTEST=`testbeg $1`

    OPTIONS="--dataPath $TMPINPUTDIR"

    sparkbenchjar $CLASS $OPTIONS
    testend $BEGTEST
}


# ==========================
# Hadoop functions
# ==========================

copy_tohdfs() {
    local -r SOURCE=$1
    local -r DESTDIR=$2
    local CMD="hdfs dfs -copyFromLocal $SOURCE $DESTDIR"
    execute_withlog ${CMD}
}

rmr_hdfs() {
    local -r DIR=$1
    local CMD="hdfs dfs -rm -r -f -skipTrash $DIR"
    execute_withlog ${CMD}
}

mkdir_hdfs() {
    local -r DIR=$1
    local CMD="hdfs dfs -mkdir -p $DIR"
    execute_withlog ${CMD}
}

yarn_job() {
    local CMD="yarn $@"
    execute_withlog ${CMD}
}

yarn_job_examples() {
    local CMD="jar $HADOOPEXAMPLES $@"
    yarn_job ${CMD}
}

hivesql() {
    local CMD="beeline -e \"$@\""
    execute_withlog ${CMD}
}

pigscript() {
    local CMD="pig -f $1"
    execute_withlog ${CMD}
}

sparkshell() {
    read -r EXECORES DRVCORES DRVMEMORY NUMEXE <<< `getconfvar spark.executor.cores spark.driver.cores spark.driver.memory spark.num.executors`
    verify_pars EXECORES DRVCORES DRVMEMORY NUMEXE

    local CMD="echo :quit | spark-shell --master yarn -i --executor-cores $EXECORES --driver-cores $DRVCORES --driver-memory $DRVMEMORY --num-executors $NUMEXE $@"
    execute_withlog ${CMD}
}

sparkbenchjar() {
    local -r CLASS=$1
    shift
    read -r EXECORES DRVCORES DRVMEMORY NUMEXE <<< `getconfvar spark.executor.cores spark.driver.cores spark.driver.memory spark.num.executors`
    verify_pars EXECORES DRVCORES DRVMEMORY NUMEXE

    local CMD="spark-submit --class org.bench.ml.$CLASS --master yarn --executor-cores $EXECORES --driver-cores $DRVCORES --driver-memory $DRVMEMORY --num-executors $NUMEXE $BENCHMARKJAR $@"
    execute_withlog ${CMD}
}

sparksql() {
    local CMD="echo exit | spark-sql --master yarn -i $@"
    execute_withlog ${CMD}
}

hadoopbenchjar() {
    read -r NUM_MAPS NUM_REDS <<< `getconfvar map.parallelism shuffle.parallelism`
    verify_pars NUM_MAPS NUM_REDS
    local -r ACTION=$1 
    shift

    local CMD="yarn jar $BENCHMARKJAR org.bench.mr.DataGen -t $ACTION -m ${NUM_MAPS} -r ${NUM_REDS} $@"
    execute_withlog ${CMD}
}

mapredtest() {
    local CMD="HADOOP_CLASSPATH=$JUNITJAR yarn jar $HADOOPMAPREDUCETEST $@"
    execute_withlog ${CMD}
}
