ENVCONF=env.conf
REPDEL='|'

# =========================
# logging
# =========================

logfile() {
    local -r MESS="$1"
}

log() {
    local -r MESS="$1"
    logfile "$MESS"
    echo $MESS
}

logfail() {
    log "$1"
    exit 4
}

execute_withlog() {
    local -r CMD="$@"
    # important: some command are assuming the first line in the output is not relevant and remove it
    # do not remove this log $CMD below
    log "$CMD"
    eval $CMD
    if [ $? -ne 0 ]; then
        # log CMD again, it can preceded by bunch of logs
        log "$CMD"
        logfail "Job failed"
    fi
}

# =============================
# temporary files
# =============================

crtemp() {
  local -r TMP=`mktemp`
  echo $TMP >>$TMPSTORE
  echo $TMP
}

# =============================
# different report functions
# =============================

dir_size() {
    local -r DIR=$1
    local -r SIZE=`hdfs dfs -du -h -s $DIR | cut -d' ' -f1-2`
    echo $SIZE
}

dir_isize() {
    echo `dir_size $TMPINPUTDIR`
}

getsec() {
  echo `date  +"%s"`
}


calculatesec() {
  local -r before=$1
  local -r after=`getsec`
  echo $(expr $after - $before)
}

getdate() {
    echo `date +"%Y-%m-%d %H-%M-%S"`
}

tsp() {
    local -r MESS="$1"
    local -r LEN=$2
    local -r OUT=`printf "%-${LEN}s $REPDEL" "$MESS"`
    echo "$OUT"
}

NAMELEN=20
DATELEN=20
SECLEN=10
SIZELEN=10


printline() {
    echo -n $REPDEL >>$REPORTFILE
    while true; do
        [ -z "$1" ] && break
        O=`tsp "$1" $2`
        echo -n "$O" >>$REPORTFILE
        shift 2
    done
    echo >>$REPORTFILE
}

testbeg() {
    local -r subtest=$1
    if [ ! -f $REPORTFILE ]; then
        printline TEST $NAMELEN SUBTEST $NAMELEN SIZE $SIZELEN START $DATELEN END $DATELEN TIME/SEC $SECLEN
    fi 
    local -r BEG="$subtest,`getdate`,`getsec`"
    echo $BEG
}

testend() {
    IFS=',' read -r subtest begdate begsec <<<$@
    printline $TESTNAME $NAMELEN $subtest $NAMELEN "`dir_isize`" $SIZELEN "$begdate" $DATELEN "`getdate`" $DATELEN "`calculatesec $begsec`" $SECLEN
}

markfailedtest() {
    local -r STATE=$1
    local -r BEGTEST=`testbeg $STATE`
    testend $BEGTEST
}


# ========================
# misc script utilties
# ========================

remove_tmpoutput() {
  rmr_hdfs $TMPOUTPUTDIR
}

remove_tmpoutput1() {
  rmr_hdfs $TMPOUTPUT1DIR
}


remove_tmp() {
  rmr_hdfs $TMPBASEDIR
}

required_var() {
    local -r VARIABLE=$1
    [ -z "${!VARIABLE}" ] && logfail "Need to set environment variable $VARIABLE"    
}

required_listofvars() {
    while true; do
        var=$1
        [ -z "$var" ] && break
        required_var $var
        shift
    done
}

required_par() {
    local -r PAR=$1
    [ -z "${!PAR}" ] && logfail "$PAR not set in $ENVCONF"
    [ "${!PAR}" == "-" ] && logfail "$PAR not set in $ENVCONF"
}

required_listofpars() {
    while true; do
        par=$1
        [ -z "$par" ] && break
        required_par $par
        shift
    done
}

log_listofpars() {
    while true; do
        par=$1
        [ -z "$par" ] && break
        val=${!par}
        log "$par=$val"
        shift
    done
}

verify_pars() {
  required_listofpars $@
  log_listofpars $@
}  


existfile() {
    local -r FILENAME=$1
    [ -f $FILENAME ] || logfail "$FILENAME does not exist"
}

existexefile() {
    local -r FILENAME=$1
    [ -x $FILENAME ] || logfail "$FILENAME is not executable"
}

existdir() {
    local -r DIRNAME=$1
    [ -d $DIRNAME ] || logfail "$DIRNAME does not exist"
}


setenv() {
    source $ENVRC
    source $CUSTOMRC
}

onthelist() {
    local -r word=$1
    local -r list=$2
    for w in ${list//,/ }; do
        [ $w == $word ] &&  return
    done
    logfail "$word is not on the list $list"
}

getconfvar() {
    declare -A arr

    # read file line by line. Field separator is "="
    while IFS='=' read -r k v; do
        [ -z "$k" ] && continue
        [ -z "$v" ] && continue
        arr[$k]=$v
    done <$ENVCONF

    res=""
    while true; do
        key=$1
        [ -z "$key" ] && break
        val=${arr[$BENCHSIZE.$key]}
        if [ -z "$val" ]; then val=${arr[$key]}; fi

        # check global replaced
        if [ -z "$val" ]; then 
          # replace . with _
          read EVAR <<< `echo $BENCHSIZE.$key | tr . _`
          val="${!EVAR}"        
        fi

        # check global not replace variable
        if [ -z "$val" ]; then 
          # replace . with _
          read EVAR <<< `echo $key | tr . _`
          val="${!EVAR}"        
        fi
        if [ -z "$val" ]; then logfile "$1 variable not found in $ENVCONF or $EVAR does not exist"; val="-";  fi
        res="$res $val"
        shift
    done
    echo $res
}

# ===========================
# repeating spark
# ===========================

prepare_sql() {
    local -r BEGTEST=`testbeg prepare`
    read -r PAGES USERVISITS <<< `getconfvar pages uservisits`
    verify_pars PAGES USERVISITS

    HIVE_BASE_HDFS=$TMPBASEDIR/hive
    HIVE_INPUT=$TMPINPUTDIR

    OPTIONS="hive \
        -b ${HIVE_BASE_HDFS} \
        -n $HIVE_INPUT \
        -p $PAGES \
        -v $USERVISITS \
        -o sequence"

 hadoopbenchjar $OPTIONS

 testend $BEGTEST    
}


spark_prepare() {
    local -r CLASS=$1
    local -r BEGTEST=`testbeg prepare`
    read -r NUM_EXAMPLES_LINEAR NUM_FEATURES_LINEAR <<< `getconfvar examples features`
    verify_pars NUM_EXAMPLES_LINEAR NUM_FEATURES_LINEAR

    OPTIONS="--numExamples $NUM_EXAMPLES_LINEAR \
             --numFeatures $NUM_FEATURES_LINEAR \
             --dataPath $TMPINPUTDIR
            "
    sparkbenchjar $1 $OPTIONS
    testend $BEGTEST
}

spark_run() {
    local -r NAME=$1
    local -r CLASS=$2
    local -r BEGTEST=`testbeg $1`

    OPTIONS="--dataPath $TMPINPUTDIR"

    sparkbenchjar $CLASS $OPTIONS
    testend $BEGTEST
}


# ==========================
# Hadoop functions
# ==========================

copy_tohdfs() {
    local -r SOURCE=$1
    local -r DESTDIR=$2
    local CMD="hdfs dfs -copyFromLocal $SOURCE $DESTDIR"
    execute_withlog ${CMD}
}

rmr_hdfs() {
    local -r DIR=$1
    local CMD="hdfs dfs -rm -r -f -skipTrash $DIR"
    execute_withlog ${CMD}
}

mkdir_hdfs() {
    local -r DIR=$1
    local CMD="hdfs dfs -mkdir -p $DIR"
    execute_withlog ${CMD}
}

yarn_job() {
    local CMD="yarn $@"
    execute_withlog ${CMD}
}

yarn_job_examples() {
    local CMD="jar $HADOOPEXAMPLES $@"
    yarn_job ${CMD}
}

hivesql() {
    local CMD="beeline -e \"$@\" $HIVEAUTH"
    execute_withlog ${CMD}
}

hiveexporttable() {
    local -r TABLE=$1
    local -r DIR=$2
    local -r WHAT=$3
    # block wildcard expansion
    set -f
    local -r SQL='select * from $TABLE'
    local CMD="beeline --outputformat=csv2 -e \" $SQL \" $HIVEAUTH "
    # remove first 2 lines
    if [ $WHAT = "dir" ]; then
        local -r P=$PWD
        cd $DIR
        execute_withlog "$CMD" | sed 1,2d | split - -l 2000000
        cd $P
    fi  
    [ $WHAT = "file" ] && execute_withlog "$CMD" | sed 1,2d >$DIR 
    set +f
}


phoenix_import() {
   local -r CSVFILE=$1
   local -t PHOTABLE=$2
   local CMD="HADOOP_CLASSPATH=/etc/hbase/conf yarn jar $PHOENIXDIR/phoenix-client.jar org.apache.phoenix.mapreduce.CsvBulkLoadTool  -d ',' --table $PHOTABLE --input $CSVFILE"
   execute_withlog ${CMD}
}   

phoenix_script() {
#    local CMD="echo \"$1\" | $PHOENIXLINE" 
    local CMD="$PHOENIXLINE $ZOOKEEPER $1"
    set -f
    execute_withlog ${CMD}
}

phoenix_command() {
    local -r TMP=`crtemp`
    echo $1 >$TMP
    phoenix_script $TMP
}


removeit_phoenix_command() {
#    local CMD="echo \"$1\" | $PHOENIXLINE" 
    local CMD="$PHOENIXLINE $ZOOKEEPER $1"
    set -f
    execute_withlog ${CMD}
}

hive_phoeniximport() {
    local -r HIVETABLE=$1
    local -r PHOTABLE=$2
    # create temporary directory
    local -r DIR=`mktemp -d -p $TEMPDIR`
    local -r TFILE=$TMPBASEDIR/phoenix.csv
    hiveexporttable $HIVETABLE $DIR dir
    set +f
    for f in $DIR/*; do 
        rmr_hdfs $TFILE
        copy_tohdfs $f $TFILE
        phoenix_import $TFILE $PHOTABLE
    done
    rm -rf $DIR
}

hivesqlscript() {
    local CMD="beeline -f $1 $HIVEAUTH"
    execute_withlog ${CMD}
}

pigscript() {
    local CMD="pig -f $1"
    execute_withlog ${CMD}
}

# ==========================================================
# !!! sparkshell does not work in the background &
# Mystery
# it hangs forever
# =========================================================

sparkbenchjar() {
    local -r CLASS=$1
    shift
    read -r EXECORES EXEMEMORY DRVCORES DRVMEMORY NUMEXE <<< `getconfvar spark.executor.cores spark.executor.memory spark.driver.cores spark.driver.memory spark.num.executors`
    verify_pars EXECORES EXEMEMORY DRVCORES DRVMEMORY NUMEXE

    local CMD="spark-submit --class org.bench.ml.$CLASS --master yarn --executor-cores $EXECORES --executor-memory $EXEMEMORY --driver-cores $DRVCORES --driver-memory $DRVMEMORY --num-executors $NUMEXE $BENCHMARKJAR $@"
    execute_withlog ${CMD}
}

sparksql() {
    read -r EXECORES DRVCORES DRVMEMORY NUMEXE EXEMEMORY <<< `getconfvar sparksql.executor.cores sparksql.driver.cores sparksql.driver.memory sparksql.num.executors sparksql_executor_memory`
    verify_pars EXECORES DRVCORES DRVMEMORY NUMEXE EXEMEMORY

    local CMD="spark-sql --master yarn --executor-memory $EXEMEMORY --executor-cores $EXECORES --driver-cores $DRVCORES --driver-memory $DRVMEMORY --num-executors $NUMEXE -f $@"
    execute_withlog ${CMD}
}

sparksqlcommand() {
    read -r EXECORES DRVCORES DRVMEMORY NUMEXE <<< `getconfvar sparksql.executor.cores sparksql.driver.cores sparksql.driver.memory sparksql.num.executors`
    verify_pars EXECORES DRVCORES DRVMEMORY NUMEXE
    
    local CMD="spark-sql --executor-cores $EXECORES --driver-cores $DRVCORES --driver-memory $DRVMEMORY --num-executors $NUMEXE --master yarn -e \"$@\""
    execute_withlog ${CMD}
}

sqlremovetable() {
    local -r COMMAND=$1
    while true; do
        shift
        [ -z "$1" ] && break
        $COMMAND "DROP TABLE IF EXISTS $1"
    done
}

sparksqlremovetable() {
    sqlremovetable sparksqlcommand $@
}

hivesqlremovetable() {
    sqlremovetable hivesql $@
}

phoenixremovetable() {
    sqlremovetable phoenix_command $@
}

hadoopbenchjar() {
    read -r NUM_MAPS NUM_REDS <<< `getconfvar map.parallelism shuffle.parallelism`
    verify_pars NUM_MAPS NUM_REDS
    local -r ACTION=$1 
    shift

    local CMD="yarn jar $BENCHMARKJAR org.bench.mr.DataGen -t $ACTION -m ${NUM_MAPS} -r ${NUM_REDS} $@"
    execute_withlog ${CMD}
}

mapredtest() {
    local CMD="HADOOP_CLASSPATH=$JUNITJAR yarn jar $HADOOPMAPREDUCETEST $@"
    execute_withlog ${CMD}
}

# ==========================
# verify sql results
# ==========================

sql_verifynonzero() {
    local -r shellcommand=$1
    local -r table=$2
    local -r TMP=`crtemp`
    local -r TMP1=`crtemp`

    $shellcommand "SELECT 'NUMBEROFROWS:' || count(*) || ':EOF' from $table" >$TMP

    cat $TMP | grep "NUMBEROFROWS" | tr -d "\| " | cut -d ":" -f2 >$TMP1
    [ $? -eq 0 ] || logfail "Failed while extracting the result"

    # second line contains number of rows
    local -r NUMOFROWS=`cat $TMP1 | tail -n1 `

    [ ${NUMOFROWS} -gt 0 ]  || logfail "Empty result"
}

hive_verifynonzero() {
    sql_verifynonzero hivesql $1
}

spark_verifynonzero() {
    sql_verifynonzero sparksqlcommand $1
}

phoenix_verifynonzero() {
    sql_verifynonzero phoenix_command $1
}

